#Install requirements
!pip install requests spacy pandas
!python -m spacy download en_core_web_sm

#Import all necessary libraries
import requests
import pandas as pd
import time
import spacy


# Alpha Vantage API details
API_KEY = enter you own api key here #you may get the key by making your own account or till then use mine
stock_symbols = stock_symbols = [
    'AAPL', 'MSFT', 'GOOGL', 'AMZN'
]

def fetch_news_articles(symbol, api_key):
    url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={api_key}'
    response = requests.get(url)
    data = response.json()
    
     
    # Print the response to inspect its structure
    print(data)

     # Check if there's an error message in the response
    if 'Error Message' in data:
        print(f"API error for symbol: {symbol}, Error: {data['Error Message']}")
        return []


    articles = []
    # Check if 'feed' key exists
    if 'feed' in data:
        for item in data['feed']:
            articles.append({
                'title': item.get('title', 'No Title'),
                'content': item.get('summary', 'No Summary'),
                'sentiment': item.get('overall_sentiment_label', 'No Sentiment'),
                'symbol': symbol
            })
    else:
        print(f"No 'feed' key in response for symbol: {symbol}")
    
    return articles

# Fetch articles for all symbols
all_articles = []
for symbol in stock_symbols:
    articles = fetch_news_articles(symbol, API_KEY)
    all_articles.extend(articles)
    time.sleep(12)  # Respect API rate limits

df = pd.DataFrame(all_articles)





#Initialize SpaCy(an advanced NLP library used for cleaning and prep-processing text)
nlp = spacy.load('en_core_web_sm')

#Start the Pre-Processing
def preprocess_text(text):
    doc = nlp(text)
    cleaned_tokens = []

    for token in doc:
        if not token.is_stop and not token.is_punct:
            cleaned_tokens.append(token.lemma_.lower())
    
    return ' '.join(cleaned_tokens)

df['cleaned_content'] = df['content'].apply(preprocess_text)
print(df[['title', 'cleaned_content', 'sentiment']])




#Vectorize and Tokenize:
def tokenize_and_vectorize(text):
    doc = nlp(text)
    return [token.vector for token in doc]

df['vectors'] = df['cleaned_content'].apply(tokenize_and_vectorize)
print(df[['title', 'cleaned_content', 'sentiment', 'vectors']])



df.to_csv('diverse_labeled_news_articles.csv', index=False)
print("Dataset saved to diverse_labeled_news_articles.csv")